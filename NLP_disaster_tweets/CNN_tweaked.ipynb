{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set(style='whitegrid',palette='muted',font_scale=1.5)\n",
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 16,10 \n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Input, GlobalMaxPooling1D\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = pd.read_csv('train.csv',index_col='id')\n",
    "raw = raw.drop(['keyword','location'], axis=1)\n",
    "data = raw['text']\n",
    "y = raw['target'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deleting shenanigans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "#remove url\n",
    "def remove_url(text):\n",
    "    url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return url.sub(r'', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.apply(lambda x: remove_url(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove html\n",
    "def remove_html(text):\n",
    "    html = re.compile(r'<.*?>')\n",
    "    return html.sub(r'', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.apply(lambda x: remove_html(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove f...ing emojis, had to google this\n",
    "def remove_emojis(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\" #emoticons\n",
    "                               u\"\\U0001F300-\\U0001F5FF\" #symbols&pics\n",
    "                               u\"\\U0001F680-\\U0001F6FF\" #transportation pic\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\" #flags\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U000024C2-\\U0001F251\"    \n",
    "                               \"]+\", flags = re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.apply(lambda x: remove_emojis(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove punctuation\n",
    "import string\n",
    "def remove_punct(text):\n",
    "    signs = str.maketrans('', '', string.punctuation)\n",
    "    return text.translate(signs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.apply(lambda x: remove_punct(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_VOCAB_SIZE = 25000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words = MAX_VOCAB_SIZE)\n",
    "tokenizer.fit_on_texts(data.values)\n",
    "X_train = tokenizer.texts_to_sequences(data.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18104"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordidx = tokenizer.word_index\n",
    "Vlength = len(wordidx)\n",
    "Vlength #unique tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7613, 31) (7613,)\n"
     ]
    }
   ],
   "source": [
    "X_train = pad_sequences(X_train)\n",
    "y_train = y.copy()\n",
    "seq_length = X_train.shape[1]\n",
    "print(X_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tweaking the CNN model\n",
    "The goal is to get closer to t1 of .85"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(D, X = X_train, y=y_train):\n",
    "    i = Input(shape=(seq_length,))\n",
    "    x = Embedding(Vlength+1, D)(i)\n",
    "    x = Conv1D(16, 2, activation = 'relu')(x)\n",
    "    x = MaxPooling1D(2)(x)\n",
    "    x = Conv1D(32, 2, activation = 'relu')(x)\n",
    "    x = MaxPooling1D(2)(x)\n",
    "    x = Conv1D(64, 2, activation = 'relu')(x)\n",
    "#    x = MaxPooling1D(1)(x)\n",
    "#    x = Conv1D(128, 1, activation = 'relu')(x)\n",
    "    x = GlobalMaxPooling1D()(x)\n",
    "    x = Dense(1, activation = 'sigmoid')(x)\n",
    "\n",
    "    model = Model(i,x)\n",
    "    model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "    ES = keras.callbacks.EarlyStopping(monitor = 'val_accuracy', patience = 2, restore_best_weights = True)\n",
    "    model.fit(X, y, epochs = 20, callbacks = [ES], validation_split = 0.1, verbose = 2)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "215/215 - 1s - loss: 0.6058 - accuracy: 0.6659 - val_loss: 0.4971 - val_accuracy: 0.7585\n",
      "Epoch 2/20\n",
      "215/215 - 1s - loss: 0.3419 - accuracy: 0.8575 - val_loss: 0.4659 - val_accuracy: 0.7822\n",
      "Epoch 3/20\n",
      "215/215 - 1s - loss: 0.1776 - accuracy: 0.9365 - val_loss: 0.5212 - val_accuracy: 0.7835\n",
      "Epoch 4/20\n",
      "215/215 - 1s - loss: 0.1074 - accuracy: 0.9641 - val_loss: 0.6059 - val_accuracy: 0.7533\n",
      "Epoch 5/20\n",
      "215/215 - 1s - loss: 0.0785 - accuracy: 0.9746 - val_loss: 0.6339 - val_accuracy: 0.7717\n"
     ]
    }
   ],
   "source": [
    "model = train_model(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "215/215 - 6s - loss: 0.5952 - accuracy: 0.6706 - val_loss: 0.4656 - val_accuracy: 0.7808\n",
      "Epoch 2/20\n",
      "215/215 - 6s - loss: 0.3298 - accuracy: 0.8675 - val_loss: 0.4503 - val_accuracy: 0.7992\n",
      "Epoch 3/20\n",
      "215/215 - 6s - loss: 0.1587 - accuracy: 0.9441 - val_loss: 0.5275 - val_accuracy: 0.7808\n",
      "Epoch 4/20\n",
      "215/215 - 6s - loss: 0.0889 - accuracy: 0.9705 - val_loss: 0.5912 - val_accuracy: 0.7533\n"
     ]
    }
   ],
   "source": [
    "model2 = train_model(128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering the texts and getting rid of all the redundant stuff actually pushes the CNN an extra 0.03\n",
    "\n",
    "Will this lead to better performance with the test set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline(data):\n",
    "    data = data.apply(lambda x: remove_url(x))\n",
    "    data = data.apply(lambda x: remove_html(x))\n",
    "    data = data.apply(lambda x: remove_emojis(x))\n",
    "    data = data.apply(lambda x: remove_punct(x))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_test = pd.read_csv('test.csv', index_col='id')\n",
    "raw_test = raw_test.drop(['keyword','location'], axis=1)\n",
    "test_data = raw_test['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pipeline(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = tokenizer.texts_to_sequences(test_data.values)\n",
    "X_test = pad_sequences(X_test, maxlen = seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred7 = [1 if n >= 0.5 else 0 for n in model.predict(X_test)]\n",
    "pred8 = [1 if n >= 0.5 else 0 for n in model2.predict(X_test)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission7 = pd.DataFrame(index = raw_test.index, data = pred7)\n",
    "submission8 = pd.DataFrame(index = raw_test.index, data = pred8)\n",
    "submission7.columns = ['target']\n",
    "submission8.columns = ['target']\n",
    "submission7.to_csv('sub7.csv')\n",
    "submission8.to_csv('sub8.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.79589!\n",
    "Scored by sub8\n",
    "\n",
    "Definitely an improvement for CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Small experiment with thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_exp = [1 if n >= 0.6 else 0 for n in model2.predict(X_test)]\n",
    "submission9 = pd.DataFrame(index = raw_test.index, data = pred_exp)\n",
    "submission9.columns = ['target']\n",
    "submission9.to_csv('sub9.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.79160 \n",
    "Fooling around with the threshold value might be a way to improve this particular model..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
